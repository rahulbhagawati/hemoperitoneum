{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e86b8ac-809b-4f48-b4fa-99c3d9dde097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom as py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, confusion_matrix, precision_score, recall_score, accuracy_score, f1_score, \n",
    "roc_auc_score, balanced_accuracy_score, matthews_corrcoef\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats import gmean\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e40d5d4-d77c-4040-b1f9-3d61833a6d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "root= \"the root/path of the folders\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30e83d9-9eaf-42ba-8ce0-e7e713e12b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load training, validation and testing data\n",
    "\n",
    "xtrain_df= pd.read_csv(root+ \"classification/\"+ \"xtrain_df.csv\")\n",
    "ytrain= np.load(root+ \"classification/\"+ \"ytrain.npy\",)\n",
    "print(\"Done\")\n",
    "\n",
    "\n",
    "xval_df= pd.read_csv(root+ \"classification/\"+ \"xval_df.csv\")\n",
    "yval= np.load(root+ \"classification/\"+ \"yval.npy\")\n",
    "print(\"Done\")\n",
    "\n",
    "\n",
    "xtest_df= pd.read_csv(root+ \"classification/\"+ \"xtest_df.csv\")\n",
    "ytest= np.load(root+ \"classification/\"+ \"ytest.npy\")\n",
    "print(\"Done\")\n",
    "\n",
    "print(f\"\\nTraining size= {xtrain_df.shape}\")\n",
    "print(f\"Validation size= {xval_df.shape}\")\n",
    "print(f\"Testing size= {xtest_df.shape}\")\n",
    "\n",
    "print(f\"\\nytrain= {np.unique(ytrain, return_counts= True)}\")\n",
    "print(f\"yval= {np.unique(yval, return_counts= True)}\")\n",
    "print(f\"ytest= {np.unique(ytest, return_counts= True)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f5d4a6-57f0-4cb5-8bc6-1395c4a82bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the final datasets\n",
    "\n",
    "xtrain_reduced_rfecv= pd.read_csv(root+ \"classification/xtrain_reduced_rfecv.csv\")\n",
    "xval_reduced_rfecv= pd.read_csv(root+ \"classification/xval_reduced_rfecv.csv\")\n",
    "xtest_reduced_rfecv= pd.read_csv(root+ \"classification/xtest_reduced_rfecv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1217a0-fd84-4430-9b0a-6e8bdc990108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the custom scorers\n",
    "\n",
    "# Custom scorer for True Negative Rate (TNR)\n",
    "def tnr(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "\n",
    "\n",
    "mcc_scorer= make_scorer(matthews_corrcoef)\n",
    "balanced_accuracy_scorer= make_scorer(balanced_accuracy_score)\n",
    "precision_scorer = make_scorer(precision_score)\n",
    "recall_scorer = make_scorer(recall_score)\n",
    "f1_scorer= make_scorer(f1_score)\n",
    "roc_auc_scorer= make_scorer(roc_auc_score)\n",
    "tnr_scorer = make_scorer(tnr)\n",
    "\n",
    "\n",
    "# CCS function\n",
    "def compute_ccs(metrics_dict):\n",
    "    selected_metrics = [\n",
    "        metrics_dict[\"Balanced Accuracy\"],\n",
    "        metrics_dict[\"F1 Score\"],\n",
    "        metrics_dict[\"ROC_AUC\"],\n",
    "        metrics_dict[\"MCC\"]\n",
    "    ]\n",
    "    return gmean([max(0, metric) for metric in selected_metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a907c4b-2591-448e-8961-dd8b30a65b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                                #Logistic Regression\n",
    "\n",
    "\n",
    "def log_reg(train_x, train_y, val_x, val_y, test_x, test_y, solver):\n",
    "\n",
    "    # Store CCS scores across multiple random runs\n",
    "    val_ccs_scores = []\n",
    "    test_ccs_scores= []\n",
    "\n",
    "    \n",
    "    for seed in range(20):  # Loop over 20 different random states\n",
    "        model = LogisticRegression(solver= solver, max_iter=10000, random_state= seed, n_jobs= -1, class_weight= \"balanced\")\n",
    "        model.fit(train_x, train_y)\n",
    "\n",
    "        # Validate\n",
    "        val_predictions = model.predict(val_x)\n",
    "        val_probabilities = model.predict_proba(val_x)[:, 1]  # For ROC AUC\n",
    "\n",
    "        val_metrics = {\n",
    "            \"Balanced Accuracy\": balanced_accuracy_score(val_y, val_predictions),\n",
    "            \"F1 Score\": f1_score(val_y, val_predictions),\n",
    "            \"ROC_AUC\": roc_auc_score(val_y, val_probabilities),\n",
    "            \"MCC\": matthews_corrcoef(val_y, val_predictions)\n",
    "        }\n",
    "\n",
    "        val_ccs = (compute_ccs(val_metrics)*100)\n",
    "        val_ccs_scores.append(val_ccs)\n",
    "\n",
    "\n",
    "        # Testing\n",
    "        test_predictions = model.predict(test_x)\n",
    "        test_probabilities = model.predict_proba(test_x)[:, 1]  # For ROC AUC\n",
    "\n",
    "        test_metrics = {\n",
    "            \"Balanced Accuracy\": balanced_accuracy_score(test_y, test_predictions),\n",
    "            \"F1 Score\": f1_score(test_y, test_predictions),\n",
    "            \"ROC_AUC\": roc_auc_score(test_y, test_probabilities),\n",
    "            \"MCC\": matthews_corrcoef(test_y, test_predictions)\n",
    "        }\n",
    "\n",
    "        test_ccs = (compute_ccs(test_metrics)*100)\n",
    "        test_ccs_scores.append(test_ccs)\n",
    "\n",
    "\n",
    "    # Print final CCS\n",
    "    print(\"\\n*******************************************************\")\n",
    "    print(f\"Validation CCS = {val_ccs_scores}\")\n",
    "    print(f\"Mean validation CCS = {round(np.mean(val_ccs_scores), 3)}\")\n",
    "\n",
    "    print(f\"\\nTesting CCS = {test_ccs_scores}\")\n",
    "    print(f\"Mean testing CCS = {round(np.mean(test_ccs_scores), 3)}\\n\")\n",
    "\n",
    "\n",
    "    \n",
    "    return np.array(val_ccs_scores), np.array(test_ccs_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9b3021-6a24-4143-b76e-dc56d15b3273",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Gaussian Naive Bayes\n",
    "\n",
    "def gnb(train_x, train_y, val_x, val_y, test_x, test_y):\n",
    "    \n",
    "    # Store CCS scores across multiple random runs\n",
    "    val_ccs_scores = []\n",
    "    test_ccs_scores= []\n",
    "\n",
    "    \n",
    "    for seed in range(20):  # Loop over 20 different random states\n",
    "        model = GaussianNB()\n",
    "        model.fit(train_x, train_y)\n",
    "\n",
    "        # Validate\n",
    "        val_predictions = model.predict(val_x)\n",
    "        val_probabilities = model.predict_proba(val_x)[:, 1]  # For ROC AUC\n",
    "\n",
    "        val_metrics = {\n",
    "            \"Balanced Accuracy\": balanced_accuracy_score(val_y, val_predictions),\n",
    "            \"F1 Score\": f1_score(val_y, val_predictions),\n",
    "            \"ROC_AUC\": roc_auc_score(val_y, val_probabilities),\n",
    "            \"MCC\": matthews_corrcoef(val_y, val_predictions)\n",
    "        }\n",
    "\n",
    "        val_ccs = (compute_ccs(val_metrics)*100)\n",
    "        val_ccs_scores.append(val_ccs)\n",
    "\n",
    "\n",
    "        # Testing\n",
    "        test_predictions = model.predict(test_x)\n",
    "        test_probabilities = model.predict_proba(test_x)[:, 1]  # For ROC AUC\n",
    "\n",
    "        test_metrics = {\n",
    "            \"Balanced Accuracy\": balanced_accuracy_score(test_y, test_predictions),\n",
    "            \"F1 Score\": f1_score(test_y, test_predictions),\n",
    "            \"ROC_AUC\": roc_auc_score(test_y, test_probabilities),\n",
    "            \"MCC\": matthews_corrcoef(test_y, test_predictions)\n",
    "        }\n",
    "\n",
    "        test_ccs = (compute_ccs(test_metrics)*100)\n",
    "        test_ccs_scores.append(test_ccs)\n",
    "\n",
    "\n",
    "    # Print final CCS\n",
    "    print(\"\\n\")\n",
    "    print(f\"Validation CCS = {val_ccs_scores}\")\n",
    "    print(f\"Mean validation CCS = {round(np.mean(val_ccs_scores), 3)}\")\n",
    "\n",
    "    print(f\"\\nTesting CCS = {test_ccs_scores}\")\n",
    "    print(f\"Mean testing CCS = {round(np.mean(test_ccs_scores), 3)}\\n\")\n",
    "\n",
    "\n",
    "    \n",
    "    return np.array(val_ccs_scores), np.array(test_ccs_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54258c34-dd11-4d12-bcf9-fb77190a210c",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Linear Discriminant Analysis\n",
    "\n",
    "def lda(train_x, train_y, val_x, val_y, test_x, test_y, solver):\n",
    "    \n",
    "    # Store CCS scores across multiple random runs\n",
    "    val_ccs_scores = []\n",
    "    test_ccs_scores= []\n",
    "\n",
    "    \n",
    "    for seed in range(20):  # Loop over 20 different random states\n",
    "        model = LinearDiscriminantAnalysis(solver= solver)\n",
    "        model.fit(train_x, train_y)\n",
    "\n",
    "        # Validate\n",
    "        val_predictions = model.predict(val_x)\n",
    "        val_probabilities = model.predict_proba(val_x)[:, 1]  # For ROC AUC\n",
    "\n",
    "        val_metrics = {\n",
    "            \"Balanced Accuracy\": balanced_accuracy_score(val_y, val_predictions),\n",
    "            \"F1 Score\": f1_score(val_y, val_predictions),\n",
    "            \"ROC_AUC\": roc_auc_score(val_y, val_probabilities),\n",
    "            \"MCC\": matthews_corrcoef(val_y, val_predictions)\n",
    "        }\n",
    "\n",
    "        val_ccs = (compute_ccs(val_metrics)*100)\n",
    "        val_ccs_scores.append(val_ccs)\n",
    "\n",
    "\n",
    "        # Testing\n",
    "        test_predictions = model.predict(test_x)\n",
    "        test_probabilities = model.predict_proba(test_x)[:, 1]  # For ROC AUC\n",
    "\n",
    "        test_metrics = {\n",
    "            \"Balanced Accuracy\": balanced_accuracy_score(test_y, test_predictions),\n",
    "            \"F1 Score\": f1_score(test_y, test_predictions),\n",
    "            \"ROC_AUC\": roc_auc_score(test_y, test_probabilities),\n",
    "            \"MCC\": matthews_corrcoef(test_y, test_predictions)\n",
    "        }\n",
    "\n",
    "        test_ccs = (compute_ccs(test_metrics)*100)\n",
    "        test_ccs_scores.append(test_ccs)\n",
    "\n",
    "\n",
    "    # Print final CCS\n",
    "    print(\"\\n*******************************************************\\n\")\n",
    "    print(f\"Validation CCS = {val_ccs_scores}\")\n",
    "    print(f\"Mean validation CCS = {round(np.mean(val_ccs_scores), 3)}\")\n",
    "\n",
    "    print(f\"\\nTesting CCS = {test_ccs_scores}\")\n",
    "    print(f\"Mean testing CCS = {round(np.mean(test_ccs_scores), 3)}\")\n",
    "\n",
    "\n",
    "    \n",
    "    return np.array(val_ccs_scores), np.array(test_ccs_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b280edd-860e-45e7-ba95-da207f95d83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree Classifier\n",
    "\n",
    "def dtc(train_x, train_y, val_x, val_y, test_x, test_y):\n",
    "    \n",
    "    # Store CCS scores across multiple random runs\n",
    "    val_ccs_scores = []\n",
    "    test_ccs_scores= []\n",
    "\n",
    "    \n",
    "    for seed in range(20):  # Loop over 20 different random states\n",
    "        model = DecisionTreeClassifier(random_state= seed, class_weight= \"balanced\")\n",
    "        model.fit(train_x, train_y)\n",
    "\n",
    "        # Validate\n",
    "        val_predictions = model.predict(val_x)\n",
    "        val_probabilities = model.predict_proba(val_x)[:, 1]  # For ROC AUC\n",
    "\n",
    "        val_metrics = {\n",
    "            \"Balanced Accuracy\": balanced_accuracy_score(val_y, val_predictions),\n",
    "            \"F1 Score\": f1_score(val_y, val_predictions),\n",
    "            \"ROC_AUC\": roc_auc_score(val_y, val_probabilities),\n",
    "            \"MCC\": matthews_corrcoef(val_y, val_predictions)\n",
    "        }\n",
    "\n",
    "        val_ccs = (compute_ccs(val_metrics)*100)\n",
    "        val_ccs_scores.append(val_ccs)\n",
    "\n",
    "\n",
    "        # Testing\n",
    "        test_predictions = model.predict(test_x)\n",
    "        test_probabilities = model.predict_proba(test_x)[:, 1]  # For ROC AUC\n",
    "\n",
    "        test_metrics = {\n",
    "            \"Balanced Accuracy\": balanced_accuracy_score(test_y, test_predictions),\n",
    "            \"F1 Score\": f1_score(test_y, test_predictions),\n",
    "            \"ROC_AUC\": roc_auc_score(test_y, test_probabilities),\n",
    "            \"MCC\": matthews_corrcoef(test_y, test_predictions)\n",
    "        }\n",
    "\n",
    "        test_ccs = (compute_ccs(test_metrics)*100)\n",
    "        test_ccs_scores.append(test_ccs)\n",
    "\n",
    "\n",
    "    # Print final CCS\n",
    "    print(\"\\n\")\n",
    "    print(f\"Validation CCS = {val_ccs_scores}\")\n",
    "    print(f\"Mean validation CCS = {round(np.mean(val_ccs_scores), 3)}\")\n",
    "\n",
    "    print(f\"\\nTesting CCS = {test_ccs_scores}\")\n",
    "    print(f\"Mean testing CCS = {round(np.mean(test_ccs_scores), 3)}\\n\")\n",
    "\n",
    "\n",
    "    \n",
    "    return np.array(val_ccs_scores), np.array(test_ccs_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131fcc64-2aa1-4b88-974f-f2a91c5b0b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                        #Random Forest Classifier\n",
    "                            #Hyperparameters chosen as per the highest testing CCS from the exhaustive search\n",
    "\n",
    "\n",
    "def rfc(train_x, train_y, val_x, val_y, test_x, test_y):\n",
    "    \n",
    "    # Store CCS scores across multiple random runs\n",
    "    val_ccs_scores = []\n",
    "    test_ccs_scores= []\n",
    "\n",
    "    \n",
    "    for seed in range(20):  # Loop over 20 different random states #n= 252\n",
    "        model = RandomForestClassifier(n_estimators= 200, max_depth= 25, random_state= seed, n_jobs= -1, class_weight= \"balanced\")\n",
    "        model.fit(train_x, train_y)\n",
    "\n",
    "        # Validate\n",
    "        val_predictions = model.predict(val_x)\n",
    "        val_probabilities = model.predict_proba(val_x)[:, 1]  # For ROC AUC\n",
    "\n",
    "        val_metrics = {\n",
    "            \"Balanced Accuracy\": balanced_accuracy_score(val_y, val_predictions),\n",
    "            \"F1 Score\": f1_score(val_y, val_predictions),\n",
    "            \"ROC_AUC\": roc_auc_score(val_y, val_probabilities),\n",
    "            \"MCC\": matthews_corrcoef(val_y, val_predictions)\n",
    "        }\n",
    "\n",
    "        val_ccs = (compute_ccs(val_metrics)*100)\n",
    "        val_ccs_scores.append(val_ccs)\n",
    "\n",
    "\n",
    "        # Testing\n",
    "        test_predictions = model.predict(test_x)\n",
    "        test_probabilities = model.predict_proba(test_x)[:, 1]  # For ROC AUC\n",
    "\n",
    "        test_metrics = {\n",
    "            \"Balanced Accuracy\": balanced_accuracy_score(test_y, test_predictions),\n",
    "            \"F1 Score\": f1_score(test_y, test_predictions),\n",
    "            \"ROC_AUC\": roc_auc_score(test_y, test_probabilities),\n",
    "            \"MCC\": matthews_corrcoef(test_y, test_predictions)\n",
    "        }\n",
    "\n",
    "        test_ccs = (compute_ccs(test_metrics)*100)\n",
    "        test_ccs_scores.append(test_ccs)\n",
    "\n",
    "\n",
    "    # Print final CCS\n",
    "    print(\"\\n\")\n",
    "    print(f\"Validation CCS = {val_ccs_scores}\")\n",
    "    print(f\"Mean validation CCS = {round(np.mean(val_ccs_scores), 3)}\")\n",
    "\n",
    "    print(f\"\\nTesting CCS = {test_ccs_scores}\")\n",
    "    print(f\"Mean testing CCS = {round(np.mean(test_ccs_scores), 3)}\\n\")\n",
    "\n",
    "\n",
    "    \n",
    "    return np.array(val_ccs_scores), np.array(test_ccs_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268979bd-5ab2-43c4-87d1-ef89370ca361",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                    #SVM on the training data and then evaluated on the validation data\n",
    "\n",
    "def svm(train_x, train_y, val_x, val_y, test_x, test_y, k):\n",
    "    \n",
    "    # Store CCS scores across multiple random runs\n",
    "    val_ccs_scores = []\n",
    "    test_ccs_scores= []\n",
    "\n",
    "    \n",
    "    for seed in range(20):  # Loop over 20 different random states\n",
    "        model = SVC(kernel= k, class_weight= \"balanced\", random_state= seed, verbose= False, probability= True)\n",
    "        model.fit(train_x, ytrain)\n",
    "\n",
    "        # Validate\n",
    "        val_predictions = model.predict(val_x)\n",
    "        val_probabilities = model.predict_proba(val_x)[:, 1]  # For ROC AUC\n",
    "\n",
    "        val_metrics = {\n",
    "            \"Balanced Accuracy\": balanced_accuracy_score(val_y, val_predictions),\n",
    "            \"F1 Score\": f1_score(val_y, val_predictions),\n",
    "            \"ROC_AUC\": roc_auc_score(val_y, val_probabilities),\n",
    "            \"MCC\": matthews_corrcoef(val_y, val_predictions)\n",
    "        }\n",
    "\n",
    "        val_ccs = (compute_ccs(val_metrics)*100)\n",
    "        val_ccs_scores.append(val_ccs)\n",
    "\n",
    "\n",
    "        # Testing\n",
    "        test_predictions = model.predict(test_x)\n",
    "        test_probabilities = model.predict_proba(test_x)[:, 1]  # For ROC AUC\n",
    "\n",
    "        test_metrics = {\n",
    "            \"Balanced Accuracy\": balanced_accuracy_score(test_y, test_predictions),\n",
    "            \"F1 Score\": f1_score(test_y, test_predictions),\n",
    "            \"ROC_AUC\": roc_auc_score(test_y, test_probabilities),\n",
    "            \"MCC\": matthews_corrcoef(test_y, test_predictions)\n",
    "        }\n",
    "\n",
    "        test_ccs = (compute_ccs(test_metrics)*100)\n",
    "        test_ccs_scores.append(test_ccs)\n",
    "\n",
    "\n",
    "    # Print final CCS\n",
    "    print(\"\\n\")\n",
    "    print(f\"Validation CCS = {val_ccs_scores}\")\n",
    "    print(f\"Mean validation CCS = {round(np.mean(val_ccs_scores), 3)}\")\n",
    "\n",
    "    print(f\"\\nTesting CCS = {test_ccs_scores}\")\n",
    "    print(f\"Mean testing CCS = {round(np.mean(test_ccs_scores), 3)}\\n\")\n",
    "\n",
    "\n",
    "    \n",
    "    return np.array(val_ccs_scores), np.array(test_ccs_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ab55d1-2842-4c39-80b1-54d285387112",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                #Light Gradient Boosting Machine\n",
    "                        #Hyperparameters chosen as per the highest testing CCS from the exhaustive search\n",
    "\n",
    "def lgbm(train_x, train_y, val_x, val_y, test_x, test_y):\n",
    "    \n",
    "    # Store CCS scores across multiple random runs\n",
    "    val_ccs_scores = []\n",
    "    test_ccs_scores= []\n",
    "\n",
    "    \n",
    "    for seed in range(20):  # Loop over 20 different random states\n",
    "        model = LGBMClassifier(n_estimators= 350, learning_rate= 0.2, random_state= seed, n_jobs= -1, class_weight= \"balanced\", verbose=-1)\n",
    "        model.fit(train_x, train_y)\n",
    "\n",
    "        # Validate\n",
    "        val_predictions = model.predict(val_x)\n",
    "        val_probabilities = model.predict_proba(val_x)[:, 1]  # For ROC AUC\n",
    "\n",
    "        val_metrics = {\n",
    "            \"Balanced Accuracy\": balanced_accuracy_score(val_y, val_predictions),\n",
    "            \"F1 Score\": f1_score(val_y, val_predictions),\n",
    "            \"ROC_AUC\": roc_auc_score(val_y, val_probabilities),\n",
    "            \"MCC\": matthews_corrcoef(val_y, val_predictions)\n",
    "        }\n",
    "\n",
    "        val_ccs = (compute_ccs(val_metrics)*100)\n",
    "        val_ccs_scores.append(val_ccs)\n",
    "\n",
    "\n",
    "        # Testing\n",
    "        test_predictions = model.predict(test_x)\n",
    "        test_probabilities = model.predict_proba(test_x)[:, 1]  # For ROC AUC\n",
    "\n",
    "        test_metrics = {\n",
    "            \"Balanced Accuracy\": balanced_accuracy_score(test_y, test_predictions),\n",
    "            \"F1 Score\": f1_score(test_y, test_predictions),\n",
    "            \"ROC_AUC\": roc_auc_score(test_y, test_probabilities),\n",
    "            \"MCC\": matthews_corrcoef(test_y, test_predictions)\n",
    "        }\n",
    "\n",
    "        test_ccs = (compute_ccs(test_metrics)*100)\n",
    "        test_ccs_scores.append(test_ccs)\n",
    "\n",
    "\n",
    "    # Print final CCS\n",
    "    print(\"\\n\")\n",
    "    print(f\"Validation CCS = {val_ccs_scores}\")\n",
    "    print(f\"Mean validation CCS = {round(np.mean(val_ccs_scores), 3)}\")\n",
    "\n",
    "    print(f\"\\nTesting CCS = {test_ccs_scores}\")\n",
    "    print(f\"Mean testing CCS = {round(np.mean(test_ccs_scores), 3)}\\n\")\n",
    "\n",
    "    both_ccs= []\n",
    "    both_ccs= val_ccs_scores+ test_ccs_scores\n",
    "    print(f\"\\nAverage of validation and testing CCS = {round(np.mean(both_ccs), 3)}\")\n",
    "    \n",
    "    return np.array(val_ccs_scores), np.array(test_ccs_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2617c5f-a1d1-4455-b5fc-25f913b7a666",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                                    #XGBoost\n",
    "                                #Hyperparameters chosen as per the highest testing CCS from the exhaustive search\n",
    "\n",
    "\n",
    "def xgb(train_x, train_y, val_x, val_y, test_x, test_y):\n",
    "    \n",
    "    # Store CCS scores across multiple random runs\n",
    "    val_ccs_scores = []\n",
    "    test_ccs_scores= []\n",
    "\n",
    "    \n",
    "    for seed in range(20):  # Loop over 20 different random states\n",
    "        model = XGBClassifier(n_estimators= 275, learning_rate= 0.2, random_state= seed, n_jobs= -1, disable_default_eval_metric= True)\n",
    "        model.fit(train_x, train_y)\n",
    "\n",
    "        # Validate\n",
    "        val_predictions = model.predict(val_x)\n",
    "        val_probabilities = model.predict_proba(val_x)[:, 1]  # For ROC AUC\n",
    "\n",
    "        val_metrics = {\n",
    "            \"Balanced Accuracy\": balanced_accuracy_score(val_y, val_predictions),\n",
    "            \"F1 Score\": f1_score(val_y, val_predictions),\n",
    "            \"ROC_AUC\": roc_auc_score(val_y, val_probabilities),\n",
    "            \"MCC\": matthews_corrcoef(val_y, val_predictions)\n",
    "        }\n",
    "\n",
    "        val_ccs = (compute_ccs(val_metrics)*100)\n",
    "        val_ccs_scores.append(val_ccs)\n",
    "\n",
    "\n",
    "        # Testing\n",
    "        test_predictions = model.predict(test_x)\n",
    "        test_probabilities = model.predict_proba(test_x)[:, 1]  # For ROC AUC\n",
    "\n",
    "        test_metrics = {\n",
    "            \"Balanced Accuracy\": balanced_accuracy_score(test_y, test_predictions),\n",
    "            \"F1 Score\": f1_score(test_y, test_predictions),\n",
    "            \"ROC_AUC\": roc_auc_score(test_y, test_probabilities),\n",
    "            \"MCC\": matthews_corrcoef(test_y, test_predictions)\n",
    "        }\n",
    "\n",
    "        test_ccs = (compute_ccs(test_metrics)*100)\n",
    "        test_ccs_scores.append(test_ccs)\n",
    "\n",
    "\n",
    "    # Print final CCS\n",
    "    print(\"\\n\")\n",
    "    print(f\"Validation CCS = {val_ccs_scores}\")\n",
    "    print(f\"Mean validation CCS = {round(np.mean(val_ccs_scores), 3)}\")\n",
    "\n",
    "    print(f\"\\nTesting CCS = {test_ccs_scores}\")\n",
    "    print(f\"Mean testing CCS = {round(np.mean(test_ccs_scores), 3)}\\n\")\n",
    "\n",
    "    both_ccs= []\n",
    "    both_ccs= val_ccs_scores+ test_ccs_scores\n",
    "    print(f\"\\nAverage of validation and testing CCS = {round(np.mean(both_ccs), 3)}\")\n",
    "    \n",
    "    return np.array(val_ccs_scores), np.array(test_ccs_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905f8692-5513-4886-9a13-b9fc4c28c380",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                        #BoostFusion: Soft voting classifier on XGB and LGBM\n",
    "\n",
    "def soft(train_x, train_y, val_x, val_y, test_x, test_y):\n",
    "    \n",
    "    # Store CCS scores across multiple random runs\n",
    "    val_ccs_scores = []\n",
    "    test_ccs_scores= []\n",
    "\n",
    "    \n",
    "    for seed in range(20):  # Loop over 20 different random states\n",
    "        model1 = XGBClassifier(n_estimators= 275, learning_rate= 0.2, random_state= seed, n_jobs= -1, disable_default_eval_metric= True)\n",
    "        model2= LGBMClassifier(n_estimators= 350, learning_rate= 0.2, random_state= seed, n_jobs= -1, class_weight= \"balanced\", verbose=-1)\n",
    "\n",
    "        # Create soft voting classifier\n",
    "        model = VotingClassifier(estimators=[('xgb', model1), ('lgbm', model2)], voting='soft')\n",
    "        \n",
    "        model.fit(train_x, train_y)\n",
    "\n",
    "        \n",
    "        # Validate\n",
    "        val_predictions = model.predict(val_x)\n",
    "        val_probabilities = model.predict_proba(val_x)[:, 1]  # For ROC AUC\n",
    "\n",
    "        val_metrics = {\n",
    "            \"Balanced Accuracy\": balanced_accuracy_score(val_y, val_predictions),\n",
    "            \"F1 Score\": f1_score(val_y, val_predictions),\n",
    "            \"ROC_AUC\": roc_auc_score(val_y, val_probabilities),\n",
    "            \"MCC\": matthews_corrcoef(val_y, val_predictions)\n",
    "        }\n",
    "\n",
    "        val_ccs = (compute_ccs(val_metrics)*100)\n",
    "        val_ccs_scores.append(val_ccs)\n",
    "\n",
    "\n",
    "        # Testing\n",
    "        test_predictions = model.predict(test_x)\n",
    "        test_probabilities = model.predict_proba(test_x)[:, 1]  # For ROC AUC\n",
    "\n",
    "        test_metrics = {\n",
    "            \"Balanced Accuracy\": balanced_accuracy_score(test_y, test_predictions),\n",
    "            \"F1 Score\": f1_score(test_y, test_predictions),\n",
    "            \"ROC_AUC\": roc_auc_score(test_y, test_probabilities),\n",
    "            \"MCC\": matthews_corrcoef(test_y, test_predictions)\n",
    "        }\n",
    "\n",
    "        test_ccs = (compute_ccs(test_metrics)*100)\n",
    "        test_ccs_scores.append(test_ccs)\n",
    "\n",
    "\n",
    "    # Print final CCS\n",
    "    print(\"\\n\")\n",
    "    print(f\"Validation CCS = {val_ccs_scores}\")\n",
    "    print(f\"Mean validation CCS = {round(np.mean(val_ccs_scores), 3)}\")\n",
    "\n",
    "    print(f\"\\nTesting CCS = {test_ccs_scores}\")\n",
    "    print(f\"Mean testing CCS = {round(np.mean(test_ccs_scores), 3)}\\n\")\n",
    "\n",
    "    both_ccs= []\n",
    "    both_ccs= val_ccs_scores+ test_ccs_scores\n",
    "    print(f\"\\nAverage of validation and testing CCS = {round(np.mean(both_ccs), 3)}\")\n",
    "    \n",
    "    return np.array(val_ccs_scores), np.array(test_ccs_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360532cd-d00b-41f2-a5bf-4d51f42ff830",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                #Statistical testing on BoostFusion\n",
    "                        #Hyperparameters chosen as per the highest testing CCS from the exhaustive search in XGB and LGBM\n",
    "\n",
    "\n",
    "val_group, test_group= soft(xtrain_reduced_rfecv, ytrain, xval_reduced_rfecv, yval, xtest_reduced_rfecv, ytest)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stat, p_value = wilcoxon(val_lgbm, val_group)\n",
    "print(f\"\\nValidation wrt LGBM:\\nStatistic = {stat:.2f}, p-value = {p_value:.4f}\")\n",
    "print(\"Mean diff:\", np.mean(np.array(val_group) - np.array(val_lgbm)))\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(f\"Significant drop in CCS (p < 0.05) -  is important.\\n\")\n",
    "else:\n",
    "    print(f\"No significant drop in CCS (p >= 0.05) -  may be less critical.\\n\")\n",
    "\n",
    "\n",
    "\n",
    "stat, p_value = wilcoxon(test_lgbm, test_group)\n",
    "print(f\"\\nTesting wrt LGBM:\\nStatistic = {stat:.2f}, p-value = {p_value:.4f}\")\n",
    "print(\"Mean diff:\", np.mean(np.array(test_group) - np.array(test_lgbm)))\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(f\"Significant drop in CCS (p < 0.05) -  is important.\\n\")\n",
    "else:\n",
    "    print(f\"No significant drop in CCS (p >= 0.05) -  may be less critical.\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stat, p_value = wilcoxon(val_xgb, val_group)\n",
    "print(f\"\\nValidation wrt XGB:\\nStatistic = {stat:.2f}, p-value = {p_value:.4f}\")\n",
    "print(\"Mean diff:\", np.mean(np.array(val_group) - np.array(val_xgb)))\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(f\"Significant drop in CCS (p < 0.05) -  is important.\\n\")\n",
    "else:\n",
    "    print(f\"No significant drop in CCS (p >= 0.05) -  may be less critical.\\n\")\n",
    "\n",
    "\n",
    "stat, p_value = wilcoxon(test_xgb, test_group)\n",
    "print(f\"\\nTesting wrt XGB:\\nStatistic = {stat:.2f}, p-value = {p_value:.4f}\")\n",
    "print(\"Mean diff:\", np.mean(np.array(test_group) - np.array(test_xgb)))\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(f\"Significant drop in CCS (p < 0.05) -  is important.\\n\")\n",
    "else:\n",
    "    print(f\"No significant drop in CCS (p >= 0.05) -  may be less critical.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc567a19-7461-42e7-a757-1049f0392ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02604297-a069-4d7d-aa34-5bc555949321",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
